{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "\n",
    "Lauren F. Klein wrote version 1.0 of this notebook, based on lessons by [Alison Parrish](http://www.decontextualize.com/) and [Jinho Choi](https://github.com/emory-courses/data-science/blob/master/course/data_aggregation/data_aggregation.ipynb), which I have supplemented with material written by [Melanie Walsh](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Collecting-Cultural-Data/Web-Scraping.html)\n",
    "\n",
    "*Quick discussion of plaforms (again) and file formats. We'll leave character encoding to next class.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download relevant files here](https://melaniewalsh.org/Web-Scraping.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do We Need To Scrape At All?\n",
    "\n",
    "To perform practical approaches to data science with text, we need...**text**. Some text is prepared for computational analysis and publicly available in digital libraries. We can easily get novels in the public domain as .txt files, for example, from [Project Gutenberg](https://www.gutenberg.org/). Or we can work with text from HathiTrust's vast collections through the [HathiTrust Research Center](https://analytics.hathitrust.org/). \n",
    "\n",
    "If we want to work with text from popular culture and the internet--and we do!--we need to scrape the web. To understand the necessity and significance of web scraping, let's walk through the likely data collection process behind [“Film Dialogue from 2,000 screenplays, Broken Down by Gender and Age”](https://pudding.cool/2017/03/film-dialogue/) or any project similar to it.\n",
    "\n",
    "One of the biggest sources for *The Pudding*'s screenplay data was the [Cornell Movie Dialogues Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). This is a corpus created by Cornell CIS professors Cristian Danescu-Niculescu-Mizil and Lillian Lee for their paper [\"Chameleons in imagined conversations\"](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf). These researchers helpfully shared a dataset of every URL that they used to find and access the screenplays in their own project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import pandas, a Python library for managing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we read in the data, which is stored as a .csv file, and which you should have downloaded at the beginning of this notebook. You'll see three \"arguments\" between parentheses in the code below. The first is the filepath, telling pandas where to find the data. You might have to edit this argument to match where you've stored the data on your computer. Don't worry about the next two arguments for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv(\"../data/cornell-movie-corpus/raw_script_urls.csv\", delimiter='\\t', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the code naming the data we've just read in, Jupyter will render the top and bottom five rows from the .csv for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>script_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>m0</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>http://www.dailyscript.com/scripts/10Things.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>m1</td>\n",
       "      <td>1492: conquest of paradise</td>\n",
       "      <td>http://www.hundland.org/scripts/1492-Conquest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>m2</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>http://www.dailyscript.com/scripts/15minutes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>m3</td>\n",
       "      <td>2001: a space odyssey</td>\n",
       "      <td>http://www.scifiscripts.com/scripts/2001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>m4</td>\n",
       "      <td>48 hrs.</td>\n",
       "      <td>http://www.awesomefilm.com/script/48hours.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>m612</td>\n",
       "      <td>watchmen</td>\n",
       "      <td>http://www.scifiscripts.com/scripts/wtchmn.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>m613</td>\n",
       "      <td>xxx</td>\n",
       "      <td>http://www.dailyscript.com/scripts/xXx.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>m614</td>\n",
       "      <td>x-men</td>\n",
       "      <td>http://www.scifiscripts.com/scripts/xmenthing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>m615</td>\n",
       "      <td>young frankenstein</td>\n",
       "      <td>http://www.horrorlair.com/scripts/young.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>m616</td>\n",
       "      <td>zulu dawn</td>\n",
       "      <td>http://www.aellea.com/script/zuludawn.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                   movie_title  \\\n",
       "0      m0    10 things i hate about you    \n",
       "1      m1    1492: conquest of paradise    \n",
       "2      m2                    15 minutes    \n",
       "3      m3         2001: a space odyssey    \n",
       "4      m4                       48 hrs.    \n",
       "..     ...                           ...   \n",
       "612  m612                      watchmen    \n",
       "613  m613                           xxx    \n",
       "614  m614                         x-men    \n",
       "615  m615            young frankenstein    \n",
       "616  m616                     zulu dawn    \n",
       "\n",
       "                                            script_url  \n",
       "0     http://www.dailyscript.com/scripts/10Things.html  \n",
       "1     http://www.hundland.org/scripts/1492-Conquest...  \n",
       "2     http://www.dailyscript.com/scripts/15minutes....  \n",
       "3         http://www.scifiscripts.com/scripts/2001.txt  \n",
       "4        http://www.awesomefilm.com/script/48hours.txt  \n",
       "..                                                 ...  \n",
       "612     http://www.scifiscripts.com/scripts/wtchmn.txt  \n",
       "613         http://www.dailyscript.com/scripts/xXx.txt  \n",
       "614   http://www.scifiscripts.com/scripts/xmenthing...  \n",
       "615        http://www.horrorlair.com/scripts/young.txt  \n",
       "616          http://www.aellea.com/script/zuludawn.txt  \n",
       "\n",
       "[617 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting HTML's anatomy with Developer Tools\n",
    "\n",
    "Thanks to Alison Parrish, we have a very simple example of HTML to begin with. It is about kittens. [Here's the rendered version](http://static.decontextualize.com/kittens.html), and [here's the HTML source code](https://raw.githubusercontent.com/ledeprogram/courses/master/databases/data/kittens.html).\n",
    "\n",
    "First we're going to use Developer Tools in Chrome to take a look at how `kittens.html` is organized. Click on the \"rendered version\" link above. In Chrome, ctrl-click (or right click) anywhere on the page and select \"Inspect Element.\" This will open Chrome's Developer Tools. Your screen should look (something) like this:\n",
    "\n",
    "<a href=\"http://static.decontextualize.com/snaps/kittens-dev-tools.png\"><img src=\"http://static.decontextualize.com/snaps/kittens-dev-tools.png\" alt=\"kittens-dev-tools\"/></a>\n",
    "\n",
    "In the upper panel, you see the web page you're inspecting. In the lower panel, you see a version of the HTML source code, with little arrows next to some of the lines. (The little arrows allow you to collapse parts of the HTML source that are hierarchically related.) As you move your mouse over the elements in the top panel, different parts of the source code will be highlighted. Chrome is showing you which parts of the source code are causing which parts of the page to show up. Pretty spiffy!\n",
    "\n",
    "This relationship also works in reverse: you can move your mouse over some part of the source code in the lower panel, which will highlight in the top panel what that source code corresponds to on the page. We'll be using this later to visually identify the parts of the page that are interesting to us, so we can write code that extracts the contents of those parts automatically.\n",
    "\n",
    "### Characterizing the structure of kittens\n",
    "\n",
    "Here's what the source code of kittens.html looks like:\n",
    "\n",
    "\t<!doctype html>\n",
    "\t<html>\n",
    "\t  <head>\n",
    "\t    <title>Kittens!</title>\n",
    "\t  </head>\n",
    "\t  <body>\n",
    "\t    <h1>Kittens and the TV Shows They Love</h1>\n",
    "\t    <div class=\"kitten\">\n",
    "\t      <h2>Fluffy</h2>\n",
    "\t      <div><img src=\"http://placekitten.com/100/100\"></div>\n",
    "\t      <ul class=\"tvshows\">\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0106145/\">Deep Space Nine</a></li>\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0088576/\">Mr. Belvedere</a></li>\n",
    "\t      </ul>\n",
    "\t      Last check-up: <span class=\"lastcheckup\">2014-01-17</span>\n",
    "\t    </div>\n",
    "\t    <div class=\"kitten\">\n",
    "\t      <h2>Monsieur Whiskeurs</h2>\n",
    "\t      <div><img src=\"http://placekitten.com/150/100\"></div>\n",
    "\t      <ul class=\"tvshows\">\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0106179/\">The X-Files</a></li>\n",
    "\t        <li><a href=\"http://www.imdb.com/title/tt0098800/\">Fresh Prince</a></li>\n",
    "\t      </ul>\n",
    "\t      Last check-up: <span class=\"lastcheckup\">2013-11-02</span>\n",
    "\t    </div>\n",
    "\t  </body>\n",
    "\t</html>\n",
    "\n",
    "This is pretty well organized HTML, but if you don't know how to read HTML, it will still look like a big jumble. Here's how I would characterize the structure of this HTML, reading in my own idea of what the meaning of the elements are.\n",
    "\n",
    "* We have two \"kittens,\" both of which are contained in `<div>` tags with class `kitten`.\n",
    "* Each \"kitten\" `<div>` has an `<h2>` tag with that kitten's name.\n",
    "* There's an image for each kitten, specified with an `<img>` tag.\n",
    "* Each kitten has a list (a `<ul>` with class `tvshows`) of television shows, contained within `<li>` tags.\n",
    "* Those list items themselves have links (`<a>` tags) with an `href` attribute that contains a link to an IMDB entry for that show.\n",
    "\n",
    "**SOME HTML QUESTIONS FOR YOU:**\n",
    "* What's the parent tag of `<a href=\"http://www.imdb.com/title/tt0088576/\">Mr. Belvedere</a>`? \n",
    "\n",
    "* Both `<div class=\"kitten\">` tags share a parent tag---what is it? What attributes are present on both `<img>` tags?\n",
    "\n",
    "### Scraping kittens with Beautiful Soup\n",
    "\n",
    "We've examined `kittens.html` a bit now. What we'd like to do is write some code that is going to extract information from the HTML, like \"what is the last checkup date for each of these kittens?\" or \"what are Monsieur Whiskeur's favorite TV shows?\" To do so, we need to *parse* the HTML, and create a representation of it in our program that we can manipulate with Python.\n",
    "\n",
    "As mentioned earlier, HTML is hard to parse by hand. (Don't even try it. In particular, [don't parse HTML with regular expressions](http://stackoverflow.com/a/1732454).)\n",
    "\n",
    "[Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) is a Python library that parses HTML (even if it's poorly formatted) and allows us to extract and manipulate its contents. More specifically, it gives us some Python objects that we can call methods on to poke at the data contained therein. So instead of working with strings and bytes, we can work with Python objects, methods, and data structures.\n",
    "\n",
    "If you're using Anaconda, Beautiful Soup comes pre-installed, so there's no extra installation step! (We will learn how to install new libraries in future classes). \n",
    "\n",
    "Note that Beautiful Soup only *parses* HTML. It's left to us to actually *get* the HTML from somewhere. In most cases, we'll want to download the HTML directly from the actual web. For that, we'll use the `get` method from the Python library `requests` ([link](https://2.python-requests.org/en/master/)):\n",
    "\n",
    "**Side note:** Since you're familiar with R, you know what a function is: a block of organized, reusable code that is called by a name, and performs some sort of action. Python has functions too, as well as things called methods, which are functions that are associated only with a particular object or class. Keep on reading to see an example of the \n",
    "`get` method in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's import the requests library\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it worked, you won't get an error message. You will see that the execution counter has been incremented by 1.\n",
    "\n",
    "Now let's use the \"get\" method to make an http request (the eponymous \"requests\") to get the contents of kittens.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"http://static.decontextualize.com/kittens.html\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"resp\" is a Python object, and not plain text.\n",
    "\n",
    "Also note that the \"get\" method makes things easy by guessing at the document's character encoding. \n",
    "\n",
    "We're not really going to talk about character encoding until next class, but since we can, let's check this page's encoding real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a string with the contents of the web page in text format we use the \"text\" method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a mess but it's apparent that we've obtained the data as desired.\n",
    "\n",
    "Now we need to create a Beautiful Soup object from that data. Here's how to go about doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "document = BeautifulSoup(html_str, \"html.parser\")\n",
    "type(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BeautifulSoup` function creates a new Beautiful Soup object. It takes two parameters: the string containing the HTML data, and a string that designates which underlying parser to use to build the parsed version of the document. (Leave this as `\"html.parser\"`.) I've assigned this object to the variable `document`. This object supports a number of interesting methods that allow us to dig into the contents of the HTML. Primarily what we'll be working with are `Tag` objects and `ResultSet` objects, which are essentially just lists of `Tag` objects.\n",
    "\n",
    "### Finding a tag\n",
    "\n",
    "As we've previously discussed, HTML documents are composed of tags. To represent this, Beautiful Soup has a type of value that represents tags. We can use the `.find()` method of the `BeautifulSoup` object to find a tag that matches a particular tag name. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_tag = document.find('h1')\n",
    "type(h1_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Tag` object has several interesting attributes and methods. The `string` attribute of a `Tag` object, for example, returns a string representing that tag's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_tag.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the attributes of a tag by treating the tag object as though it were a dictionary, using the square-bracket index syntax, with the name of the attribute whose value you want as a string inside the brackets. \n",
    "\n",
    "**NOTE FOR FOLKS USING PYTHON FOR THE FIRST TIME:**\n",
    "\n",
    "Unlike R, which (I think) just has lists, Python has two data types that store values: lists which are ordered, and store only one value, and dictionaries--what I'm talking\n",
    "about here--that store UNORDERED sets of values in key:value pairs. \n",
    "\n",
    "For more on dictionaries, see [this notebook](dictionaries-sets-tuples.ipynb).\n",
    "\n",
    "For now, though, just see if you can follow this example:\n",
    "\n",
    "To print out the `src` attribute of the first `<img>` tag in the document, you would write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tag = document.find('img')\n",
    "\n",
    "img_tag['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You might have noticed that there is more than one `<img>` tag in `kittens.html`! If more than one tag matches the name you pass to `.find()`, it returns only the *first* matching tag. (A better name for `.find()` might be `find_first`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding multiple tags\n",
    "\n",
    "It's very often the case that we want to find not just one tag that matches particular criteria, but ALL tags matching those criteria. For that, we use the `.find_all()` method of the `BeautifulSoup` object. For example, to find all `h2` tags in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_tags = document.find_all('h2')\n",
    "type(h2_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what's in the Result Set?\n",
    "\n",
    "In order to find out, we're going to need to use a loop.\n",
    "\n",
    "We're going to use some 'for' loop syntax that's very common in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in h2_tags:\n",
    "    print(tag.string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conventiently gives you a variable, `tag`, that updates with the appropriate value each time you iterate. \n",
    "\n",
    "Does R have this too? You tell me! \n",
    "\n",
    "(For more on loops and counting, see [this notebook](counting.ipynb)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, both the `.find()` and `.find_all()` methods can search not just for tags with particular names, but also for tags that have particular attributes. For that, we use the `attrs` keyword argument, giving it a dictionary that associates attribute names as keys and the desired attribute value as values. For example, to find all `span` tags with a `class` attribute of `lastcheckup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkup_tags = document.find_all('span', attrs={'class': 'lastcheckup'})\n",
    "[tag.string for tag in checkup_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note about list comprehension in Python**\n",
    "\n",
    "Line 2 above is a helpful shorthand: it creates a list (same as R lists) with each of the `tag.string`s in `checkup_tags`.\n",
    "\n",
    "In more official terms, it's called a *list comprehension*, and it helps with a very common task in both data analysis and computer programming: when you want to apply an operation to every item in a list (e.g., scaling the numbers in a list by a fixed factor), or create a copy of a list with only those items that match a particular criterion (e.g., eliminating values that fall below a certain threshold). \n",
    "\n",
    "A list comprehension has a few parts:\n",
    "\n",
    "* a source list, or the list whose values will be transformed or filtered;\n",
    "* a predicate expression, to be evaluated for every item in the list;\n",
    "* (optionally) a membership expression that determines whether or not an item in the source list will be included in the result of evaluating the list comprehension, based on whether the expression evaluates to True or False; and\n",
    "* a temporary variable name by which each value from the source list will be known in the predicate expression and membership expression.\n",
    "These parts are arranged like so:\n",
    "\n",
    "> `[` *predicate expression* `for` *temporary variable name* `in` *source list* `if` *membership expression* `]`\n",
    "\n",
    "The words for, in, and if are a part of the syntax of the expression. They don't mean anything in particular (and in fact, they do completely different things in other parts of the Python language). You just have to spell them right and put them in the right place in order for the list comprehension to work.\n",
    "\n",
    "You can see more examples of this in action [here](lists.ipynb).\n",
    "\n",
    "**An unrelated note, but before we move on:**\n",
    "\n",
    "Beautiful Soup's `.find()` and `.find_all()` methods are actually more powerful than we're letting on here. [Check out the details in the official Beautiful Soup documentation.](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
